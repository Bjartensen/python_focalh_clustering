{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8dab460-e764-41c0-b38f-c9aa57491211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import ROOT\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "\n",
    "# Add the root folder to Python path\n",
    "root_folder = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if root_folder not in sys.path:\n",
    "    sys.path.append(root_folder)\n",
    "\n",
    "import lib.unet_nn as UNet\n",
    "import lib.modified_aggregation as MA\n",
    "from lib.modified_aggregation_clusterer import ModifiedAggregationClusterer\n",
    "from lib.unet_clusterer import UNetClusterer\n",
    "from lib.focal import FocalH\n",
    "from lib.base_nn import Data\n",
    "from lib import metrics# import count_clusters, count_labels, compute_score, total, separation_efficiency\n",
    "\n",
    "os.chdir('/home/bjartur/workspace/python_focalh_clustering/') # Laptop and Desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b27269-238d-4583-b6c3-a9fa88477f7e",
   "metadata": {},
   "source": [
    "# Study overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe4b4917-0d27-441c-b2b5-f5cb13e546f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: \n",
      "{'name': 'train_single_small', 'files': [{'path': 'data/train/TRAIN_E350_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E300_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E250_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E200_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E150_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E100_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E80_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E60_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E60_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E40_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}, {'path': 'data/train/TRAIN_E20_P1_N100.root', 'particles': 1, 'saturation': 4096, 'meta': {'source': 'mc', 'detector': 'prototype2'}}]}\n",
      "its: 10\n"
     ]
    }
   ],
   "source": [
    "study_file = \"studies/study_dbscan_08092025_154943.pkl\"\n",
    "with open(study_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "#print(f\"bundle: \\n{loaded_bundle}\")\n",
    "#print()\n",
    "#print(f\"study: {loaded_bundle['study'].best_params}\")\n",
    "#print(loaded_bundle[\"timestamps\"])\n",
    "print(f\"data: \\n{loaded_bundle[\"data\"]}\")\n",
    "print(f\"its: {loaded_bundle[\"its\"]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bfb1b2-1863-48ec-846e-0ab5902fae8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.44451642036438"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_bundle[\"timestamps\"][\"t_optimize_end\"] - loaded_bundle[\"timestamps\"][\"t_optimize_start\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9db7a295-2787-43cb-a950-0bf9d8c4a79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "par1: adsf\n",
      "par2: bla\n",
      "par3: None\n"
     ]
    }
   ],
   "source": [
    "def lst_test(par1=None, par2=None, par3=None):\n",
    "    print(f\"par1: {par1}\")\n",
    "    print(f\"par2: {par2}\")\n",
    "    print(f\"par3: {par3}\")\n",
    "lst = [\"adsf\", \"bla\"]\n",
    "lst_test(*lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f8775-e25a-4ded-b1d4-2062811c495d",
   "metadata": {},
   "source": [
    "# Evaluate studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dadc6257-9cb4-4a73-b22a-e190bb2c50a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_study_file = \"studies/best/study_ma_24082025_133543.pkl\"\n",
    "\n",
    "cnn_study_file = \"studies/study_cnn_24082025_132142.pkl\"\n",
    "cnn_model_file = \"studies/model_cnn_24082025_132142.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27cd5384-ecfe-4f5c-a2e4-fe384ac4811c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'ma',\n",
       " 'study': <optuna.study.study.Study at 0x725de910a150>,\n",
       " 'config': {'analysis': {'type': 'standard',\n",
       "   'files': [{'path': 'data/E150_P5_N1000.root',\n",
       "     'particles': 5,\n",
       "     'saturation': 4096,\n",
       "     'meta': {'source': 'mc', 'detector': 'prototype2'}},\n",
       "    {'path': 'data/E150_P3_N1000.root',\n",
       "     'particles': 3,\n",
       "     'saturation': 4096,\n",
       "     'meta': {'source': 'mc', 'detector': 'prototype2'}}]}},\n",
       " 'its': 100,\n",
       " 'exec': <lib.modified_aggregation.ModifiedAggregation at 0x725deb49f350>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(ma_study_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "ma_study = loaded_bundle\n",
    "ma = MA.ModifiedAggregation(ma_study[\"study\"].best_params[\"seed\"], ma_study[\"study\"].best_params[\"agg\"])\n",
    "#ma = MA.ModifiedAggregation(1400, 0)\n",
    "ma_study[\"exec\"] = ma\n",
    "ma_study[\"study\"].best_params\n",
    "ma_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a6d396-14bb-4987-96ab-2708716a3ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 'cnn',\n",
       " 'study': <optuna.study.study.Study at 0x725dea1d5c40>,\n",
       " 'config': {'analysis': {'type': 'standard',\n",
       "   'files': [{'path': 'data/E150_P5_N1000.root',\n",
       "     'particles': 5,\n",
       "     'saturation': 4096,\n",
       "     'meta': {'source': 'mc', 'detector': 'prototype2'}},\n",
       "    {'path': 'data/E150_P3_N1000.root',\n",
       "     'particles': 3,\n",
       "     'saturation': 4096,\n",
       "     'meta': {'source': 'mc', 'detector': 'prototype2'}}]}},\n",
       " 'its': 10,\n",
       " 'exec': UNet(\n",
       "   (e11): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "   (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (upconv1): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "   (d11): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (d12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "   (d21): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (d22): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "   (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       " )}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(cnn_study_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "u = torch.load(cnn_model_file, weights_only=False)\n",
    "cnn_study = loaded_bundle\n",
    "cnn_study[\"exec\"] = u\n",
    "cnn_study[\"study\"].best_params\n",
    "cnn_study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efb0ddb-f5b5-4f38-bb09-55fdeee85228",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../p2_sim_adj_map2.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/bjartur/workspace/python_focalh_clustering/data/E300_P10_N1000.root\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[43mcluster_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_study\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m cluster_event(filename, entry, ma_study)\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mcluster_event\u001b[0;34m(tfile, entry, study)\u001b[0m\n\u001b[1;32m     36\u001b[0m     fig\u001b[38;5;241m.\u001b[39msavefig(study[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_P\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(particles)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_EV\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(entry))\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m study[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 39\u001b[0m     iadj \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../p2_sim_adj_map2.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     adj \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../p2_image_adj_21x21.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m4\u001b[39m))        \n",
      "File \u001b[0;32m~/workspace/anaconda3/envs/focalh/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../p2_sim_adj_map2.npy'"
     ]
    }
   ],
   "source": [
    "def cluster_event(tfile, entry, study):\n",
    "    \"\"\"\n",
    "    Just try clustering an event and view it.\n",
    "    \"\"\"\n",
    "#    study[\"exec\"]\n",
    "\n",
    "    SAT = 4096\n",
    "    tfile = ROOT.TFile(tfile, \"READ\")\n",
    "    ttree = tfile.Get(\"EventsTree\")\n",
    "    ttree.GetEntry(entry)\n",
    "\n",
    "    nplab = np.array(ttree.labels, dtype=np.int32)\n",
    "    npfrac = np.array(ttree.fractions, dtype=np.float32)\n",
    "    npvals = np.array(ttree.value, dtype=np.float32)\n",
    "    valmask = npvals > SAT\n",
    "    npvals[valmask] = SAT\n",
    "    particles = len(set(nplab))\n",
    "    \n",
    "    dataloader = Data()\n",
    "    npdlab = dataloader.get_major_labels(nplab, npfrac, particles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if study[\"method\"] == \"ma\":\n",
    "        iadj = np.load(\"../p2_sim_adj_map2.npy\")\n",
    "        adj = np.load(\"../p2_adj.npy\")\n",
    "        fig, ax = plt.subplots(ncols=2, figsize=(8,4))        \n",
    "        foc = FocalH()\n",
    "        foc.heatmap(npvals[iadj], npdlab[iadj], ax[0], SAT)\n",
    "        clabels,_ = study[\"exec\"].run(adj, npvals[iadj])\n",
    "        foc.heatmap(npvals[iadj], clabels, ax[1], SAT)\n",
    "        ax[0].set_title(f\"Original. N particles: {count_labels(npdlab)}\")\n",
    "        ax[1].set_title(f\"Clustered. N clusters: {count_clusters(clabels)}\")\n",
    "\n",
    "        fig.savefig(study[\"method\"]+\"_P\"+str(particles)+\"_EV\"+str(entry))\n",
    "            \n",
    "    elif study[\"method\"] == \"cnn\":\n",
    "        iadj = np.load(\"../p2_sim_adj_map2.npy\")\n",
    "        adj = np.load(\"../p2_image_adj_21x21.npy\")\n",
    "        fig, ax = plt.subplots(ncols=3, figsize=(12,4))        \n",
    "        foc = FocalH()\n",
    "        foc.heatmap(npvals[iadj], npdlab[iadj], ax[0], SAT)\n",
    "        ret, coms, dlabels, mapping = dataloader.read_ttree_event(ttree, entry)\n",
    "        #target = dataloader.gaussian_class_activation_map(coms, 21, 21, 3)[0][0]\n",
    "\n",
    "        x = study[\"exec\"](ret)[0][0]\n",
    "        vals = x.flatten().detach().numpy()\n",
    "#        vals = vals / vals.max()\n",
    "        ax[1].imshow(x.detach().numpy())\n",
    "\n",
    "        seed = study[\"study\"].best_params[\"seed\"]\n",
    "        agg = study[\"study\"].best_params[\"agg\"]\n",
    "#        clusterizer = MA.ModifiedAggregation(seed,agg)\n",
    "        clusterizer = MA.ModifiedAggregation(0.2,0)\n",
    "        clabels,_ = clusterizer.run(adj,vals)\n",
    "\n",
    "        clabels = dataloader.invert_labels(clabels, mapping, vals, npdlab.shape[0])\n",
    "#        print(vals.max())\n",
    "        foc.heatmap(npvals[iadj], clabels.astype(np.int32), ax[2], SAT)\n",
    "        ax[0].set_title(f\"Original. N particles: {count_labels(npdlab)}\")\n",
    "        ax[1].set_title(f\"Smoothed showers\")\n",
    "        ax[2].set_title(f\"Clustered. N clusters: {count_clusters(clabels)}\")\n",
    "\n",
    "        fig.savefig(study[\"method\"]+\"_P\"+str(particles)+\"_EV\"+str(entry))\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "filename = \"/home/bjartur/workspace/python_focalh_clustering/data/E300_P10_N1000.root\"\n",
    "entry = 400\n",
    "cluster_event(filename, entry, cnn_study)\n",
    "cluster_event(filename, entry, ma_study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cde7e5-0a26-42df-a382-18b63f27153d",
   "metadata": {},
   "source": [
    "# Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22435cc-4fc6-42db-9be1-bd86a2728fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbscan_study_file = \"studies/study_hdbscan_03092025_192825.pkl\"\n",
    "\n",
    "with open(hdbscan_study_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "hdbscan_study = loaded_bundle\n",
    "print(hdbscan_study[\"study\"].best_params)\n",
    "hdbscan_study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41e563-247f-4fb4-8f29-eb1deba55961",
   "metadata": {},
   "source": [
    "# Analyse study stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424bff56-9ceb-428e-b532-300557aca5f8",
   "metadata": {},
   "source": [
    "Probably a good thing to, for each study, be able to show if the parameters seem to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0a632-a13d-4c7b-a81f-411843eefb4c",
   "metadata": {},
   "source": [
    "# Curtain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b943670c-a9e3-48f3-9916-84fbe49f74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_synth = np.zeros(21*21, dtype=np.float32).reshape(21,21)\n",
    "\n",
    "fig,ax=plt.subplots(ncols=2, figsize=(10,5))\n",
    "\n",
    "size=5\n",
    "c1 = (9,9)\n",
    "b1 = slice(c1[0],c1[0]+size), slice(c1[1],c1[1]+size)\n",
    "\n",
    "\n",
    "c2 = (5,5)\n",
    "b2 = slice(c2[0],c2[0]+size), slice(c2[1],c2[1]+size)\n",
    "\n",
    "\n",
    "x_synth[b1] = 1\n",
    "x_synth[b2] = 1\n",
    "\n",
    "x_synth_tensor = torch.from_numpy(x_synth).unsqueeze(0).unsqueeze(0)\n",
    "x_synth_pred = cnn_study[\"exec\"](x_synth_tensor)\n",
    "ax[0].imshow(x_synth)\n",
    "ax[1].imshow(x_synth_pred[0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bd8a6d-ad5d-48b7-8e9a-9b2684f03908",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(nrows=2,ncols=4, figsize=(10,5))\n",
    "\n",
    "for i in range(4):\n",
    "    i_synth = np.zeros(21*21, dtype=np.float32).reshape(21,21)\n",
    "    origin = 4,4\n",
    "    size = 2*(i+1)\n",
    "    bi = slice(origin[0],origin[0]+size), slice(origin[1],origin[1]+size)\n",
    "    i_synth[bi] = 1\n",
    "    \n",
    "    i_synth_tensor = torch.from_numpy(i_synth).unsqueeze(0).unsqueeze(0)\n",
    "    i_synth_pred = cnn_study[\"exec\"](i_synth_tensor)\n",
    "    ax[0][i].imshow(i_synth)\n",
    "    ax[1][i].imshow(i_synth_pred[0][0].detach().numpy())\n",
    "\n",
    "fig.savefig(\"synth_blobs.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f6115-0c18-4dc4-83be-1502f230a0a7",
   "metadata": {},
   "source": [
    "# Analyse the magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a69b6-82fe-4d7e-b8c1-53a5c2a48e54",
   "metadata": {},
   "source": [
    "Add masks over showers and see if it's able to intelligently reconstruct what is behind the mask (saturated area).\n",
    "\n",
    "Call it curtain test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a4da0-62b0-428b-814e-d58e0172b2e3",
   "metadata": {},
   "source": [
    "# Analyse evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85451a9-efc2-44fb-a4f4-8c5dfde61094",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"font.sans-serif\": [\"DejaVu Sans\"],\n",
    "    \"font.size\" : 15,\n",
    "    \"legend.fontsize\" : 12,\n",
    "    \"xtick.direction\" : \"in\",\n",
    "    \"ytick.direction\" : \"in\",\n",
    "    \"xtick.major.size\" : 5.0,\n",
    "    \"xtick.minor.size\" : 3.0,\n",
    "    \"ytick.major.size\" : 5.0,\n",
    "    \"ytick.minor.size\" : 3.0,\n",
    "    \"axes.linewidth\" : 0.8,\n",
    "    \"legend.handlelength\" : 2,\n",
    "})\n",
    "\n",
    "def plot_discrete(x,y,label=\"\"):\n",
    "    unq = np.unique(x)\n",
    "    muy = np.zeros_like(unq)\n",
    "    mux = np.zeros_like(unq)\n",
    "    sigmay = np.zeros_like(unq)\n",
    "    sigmax = np.zeros_like(unq)\n",
    "    for i in range(len(unq)):\n",
    "        mask = x == unq[i]\n",
    "        muy[i] = y[mask].mean()\n",
    "        mux[i] = x[mask].mean()\n",
    "        sigmay[i] = y[mask].std()/np.sqrt(len(y[mask]))\n",
    "        sigmax[i] = x[mask].std()/np.sqrt(len(x[mask]))\n",
    "\n",
    "    plt.errorbar(mux,muy,sigmay,marker=\".\",linestyle=\"\", label=label)\n",
    "    plt.ylim(0,2)\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "def plot_performance_particle_num(df, x_col, y_col, marker_shape=\"^\", linestyle=\"\", color=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    grp = df.groupby(x_col)[y_col]\n",
    "    mu_x = grp.mean().index\n",
    "    mu_y = grp.mean().values\n",
    "    sigma_y = grp.sem().values\n",
    "    if color == \"\":\n",
    "        ax.errorbar(mu_x, mu_y, yerr=sigma_y, marker=marker_shape, linestyle=linestyle, capsize=5)\n",
    "    else:\n",
    "        ax.errorbar(mu_x, mu_y, yerr=sigma_y, marker=marker_shape, linestyle=linestyle, color=color, capsize=5)\n",
    "    #ax.plot(mu_x, mu_y)\n",
    "\n",
    "\n",
    "def plot_chunks(x, y, Nbins, label, marker_shape=\".\", color=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    bins = np.linspace(min(x), max(x), Nbins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    mu_y = np.zeros(Nbins)\n",
    "    sigma_y = np.zeros(Nbins)\n",
    "    \n",
    "    for i in range(Nbins):\n",
    "        mask = (x >= bins[i]) & (x < bins[i+1])\n",
    "        if np.any(mask):\n",
    "            mu_y[i] = np.mean(y[mask])\n",
    "            sigma_y[i] = np.std(y[mask]) / np.sqrt(np.sum(mask))\n",
    "\n",
    "    if color == \"\":\n",
    "        ax.errorbar(bin_centers, mu_y, yerr=sigma_y, marker=marker_shape, linestyle='solid', capsize=5)\n",
    "    else:\n",
    "        ax.errorbar(bin_centers, mu_y, yerr=sigma_y, marker=marker_shape, linestyle='solid', color=color, capsize=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_with_suffix(num):\n",
    "    if num >= 1e9:\n",
    "        return f\"{num / 1e9:.1f}B\"\n",
    "    elif num >= 1e6:\n",
    "        return f\"{num / 1e6:.1f}M\"\n",
    "    elif num >= 1e3:\n",
    "        return f\"{num / 1e3:.1f}K\"\n",
    "    else:\n",
    "        return str(num)\n",
    "\n",
    "\n",
    "#plot_discrete(count, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dccb2ea-a8e8-4e47-bfd8-1fc4eb4f14a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ma_eval_file = \"evaluation/eval_ma_01092025_053548.pkl\"\n",
    "#ma_eval_file = \"evaluation/eval_ma_07092025_103817.pkl\"\n",
    "#ma_eval_file = \"evaluation/eval_ma_06092025_102441.pkl\"\n",
    "#ma_eval_file = \"evaluation/eval_ma_08092025_092411.pkl\"\n",
    "#ma_eval_file = \"evaluation/eval_ma_08092025_200714.pkl\" # train_single_small\n",
    "#ma_eval_file = \"evaluation/eval_ma_small_many_sep_13092025_173010.pkl\" # train_small_100 sep_eff\n",
    "#ma_eval_file = \"evaluation/eval_ma_tiny_many_sep_13092025_195950.pkl\" # train_tiny_1000 sep_eff\n",
    "#ma_eval_file = \"evaluation/eval_ma_tiny_single_sep_13092025_204113.pkl\" # train_tiny_single_1000 sep_eff\n",
    "ma_eval_file = \"evaluation/eval_ma_two_15092025_130928.pkl\" # train_tiny_single_1000 sep_eff eval_two\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#cnn_eval_file = \"evaluation/eval_cnn_02092025_060217.pkl\"\n",
    "#cnn_eval_file = \"evaluation/eval_cnn_02092025_104200.pkl\"\n",
    "#cnn_eval_file = \"evaluation/eval_cnn_07092025_094926.pkl\"\n",
    "cnn_eval_file = \"evaluation/eval_cnn_07092025_102542.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "#hdbscan_eval_file = \"evaluation/eval_hdbscan_03092025_201034.pkl\"\n",
    "#hdbscan_eval_file = \"evaluation/eval_hdbscan_07092025_103658.pkl\"\n",
    "#hdbscan_eval_file = \"evaluation/eval_hdbscan_05092025_113309.pkl\"\n",
    "hdbscan_eval_file = \"evaluation/eval_hdbscan_08092025_182644.pkl\" # train_tiny\n",
    "#hdbscan_eval_file = \"evaluation/eval_hdbscan_08092025_194252.pkl\" # train_tiny smaller multiply\n",
    "\n",
    "\n",
    "\n",
    "dbscan_eval_file = \"evaluation/eval_dbscan_08092025_162047.pkl\"\n",
    "\n",
    "\n",
    "with open(cnn_eval_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "cnn_eval = loaded_bundle\n",
    "\n",
    "\n",
    "with open(ma_eval_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "ma_eval = loaded_bundle\n",
    "\n",
    "\n",
    "with open(hdbscan_eval_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "hdbscan_eval = loaded_bundle\n",
    "\n",
    "\n",
    "with open(dbscan_eval_file, \"rb\") as f:\n",
    "    loaded_bundle = pickle.load(f)\n",
    "dbscan_eval = loaded_bundle\n",
    "\n",
    "\n",
    "#cnn_eval[\"method\"]\n",
    "#cnn_eval[\"eval\"]\n",
    "\n",
    "\n",
    "#hdbscan_eval[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25671d3d-8051-4aa5-90fd-e886ff952fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_eval[\"eval\"][\"separation\"][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f06d52-ec93-4c47-9f88-3f140bb28c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_separation(d, e):\n",
    "    energies = d[\"eval\"][\"energy\"][e]\n",
    "    labels = d[\"eval\"][\"labels\"][e]\n",
    "    values = d[\"eval\"][\"values\"][e]\n",
    "    tags = d[\"eval\"][\"tags\"][e]\n",
    "    foc = FocalH()\n",
    "    iadj = np.load(\"p2_sim_adj_map2.npy\")\n",
    "    fig,ax=plt.subplots(ncols=2)\n",
    "    foc.heatmap(values[iadj], labels[iadj],ax[0])\n",
    "    foc.heatmap(values[iadj], tags[iadj],ax[1]) # EVAL IS WRONGLY NOT USING IADJ\n",
    "    separated = metrics.resolved(tags[iadj], labels[iadj], values[iadj], energies, 298.2, 7570, 0.97, 0.00, 0.08)\n",
    "#    separated = metrics.resolved(tags[iadj], labels[iadj], values[iadj], energies, 181.52, -3272.28, 0.97, 0.00, 0.08)\n",
    "#    separated = metrics.resolved(tags[iadj], labels[iadj], values[iadj], energies, 298.2, 7570, 1.18, 0.00, 0.09)\n",
    "    print(f\"Resolved: {separated[0]} out of {separated[1]}\")\n",
    "    print(f\"Efficiency: {d[\"eval\"][\"efficiency\"][e]}\")\n",
    "#    resolved(tags[iadj], labels[iadj], values[iadj], energies, 181.52, -3272, 0.97, 0.00, 0.08)\n",
    "\n",
    "\n",
    "    # 150\n",
    "test_separation(ma_eval, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b035f53-284a-41fc-b057-50209535119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "300 * np.sqrt(  (0.97**2)/300 + (0.00**2)/(300**2) + 0.09**2)\n",
    "#sigma_E = np.sqrt((0.97**2)/300 + (b**2)/(E**2) + c**2)\n",
    "\n",
    "metrics.energy_resolution(300, 0.97, 0.00, 0.09)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8ebd4-1395-4597-89c0-9504b0fdfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74976bd5-9fc7-4eea-b651-9fc977ecb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.reconstruct_energy(117446,298.2, 7570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6dfc18-7739-4dc7-92e6-c8f49aac8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(metrics)\n",
    "\n",
    "res = metrics.separation_efficiency_opt(ma_eval[\"eval\"][\"tags\"], ma_eval[\"eval\"][\"labels\"], ma_eval[\"eval\"][\"values\"], ma_eval[\"eval\"][\"energy\"])\n",
    "#from lib.metrics import count_clusters, count_labels, compute_score, total, separation_efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557483b6-bbf4-4e15-a46b-7bc21aa60520",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1077cf6-69ac-463b-be7a-239818f308fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(coms):\n",
    "    print(len(coms))\n",
    "    dists = [None]*len(coms)\n",
    "    for i in range(len(coms)):\n",
    "        diff = coms[i][:, np.newaxis, :] - coms[i][np.newaxis, :, :]\n",
    "        dist = np.sqrt((diff ** 2).sum(axis=-1))\n",
    "        upper_tri = dist[np.triu_indices_from(dist, k=1)]\n",
    "        dists[i] = upper_tri.mean()\n",
    "    return dists\n",
    "\n",
    "\n",
    "def to_dataframe(d):\n",
    "#    print(d[\"method\"])\n",
    "    df = pd.DataFrame({\n",
    "        \"efficiency\": d[\"eval\"][\"efficiency\"],\n",
    "        \"vmeasure\": d[\"eval\"][\"vmeasure\"],\n",
    "#        \"vmeasure_weighted\": d[\"vmeasure_weighted\"],\n",
    "        \"coverage\": d[\"eval\"][\"coverage\"],\n",
    "        \"particles\": d[\"eval\"][\"particles\"],\n",
    "        \"avg_energy\": d[\"eval\"][\"avg_energy\"],\n",
    "        \"coms_dists\" : dist(d[\"eval\"][\"coms\"]),\n",
    "        \"separation_resolved\" : d[\"eval\"][\"separation\"][:,0],\n",
    "        \"separation_total\" : d[\"eval\"][\"separation\"][:,1],\n",
    "                         })\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_ma = to_dataframe(ma_eval)\n",
    "#df_cnn = to_dataframe(cnn_eval)\n",
    "#df_hdbscan = to_dataframe(hdbscan_eval)\n",
    "#df_dbscan = to_dataframe(dbscan_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18872f65-c2fb-4acd-adc1-6603e59cffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(df_ma[\"efficiency\"])\n",
    "df_ma[\"efficiency\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af27b3-ebcc-4c50-8917-bc8dcb7e1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots()\n",
    "plot_performance_particle_num(df_ma, \"avg_energy\", \"efficiency\", marker_shape=\"^\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_cnn, \"avg_energy\", \"efficiency\", marker_shape=\"s\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_hdbscan, \"avg_energy\", \"efficiency\", marker_shape=\"o\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_dbscan, \"avg_energy\", \"efficiency\", marker_shape=\"o\", linestyle=\"dashed\", ax=ax)\n",
    "#ax.set_xticks([1,2,3,4,5,6,7,8,9,10], [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\n",
    "ax.set_xticks([0,50,100,150,200,250,300,350], [\"0\",\"50\",\"100\",\"150\",\"200\",\"250\",\"300\",\"350\"])\n",
    "#ax.set_xticks([1,2], [\"1\",\"2\"])\n",
    "ax.set_ylim(0,3)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "#ax.yaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_ticks_position(\"both\")\n",
    "ax.xaxis.set_ticks_position(\"both\")\n",
    "ax.set_xlabel(\"Energy [GeV]\")\n",
    "ax.set_ylabel(\"Efficiency [$N_{clusters}/N_{particles}$]\")\n",
    "ax.axhline(1, color=\"grey\", linestyle=\"dotted\")\n",
    "ax.scatter([],[], label=\"Modified Aggregation\", marker=\"^\")\n",
    "ax.scatter([],[], label=\"CNN+Modified Aggregation\", marker=\"s\")\n",
    "ax.scatter([],[], label=\"HDBSCAN\", marker=\"s\")\n",
    "ax.scatter([],[], label=\"DBSCAN\", marker=\"s\")\n",
    "ax.legend(framealpha=0)\n",
    "\n",
    "#ax.text(0.00, 1.05, \"FoCal-H Prototype 2\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\")\n",
    "#ax.text(1.00, 1.05, f\"${len(df_ma):.0E}$ MC Events\", transform=plt.gca().transAxes, ha=\"right\", va=\"center\")\n",
    "\n",
    "\n",
    "ax.text(0.03, 0.93, \"FoCal-H\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\")\n",
    "ax.text(0.03, 0.86, \"Prototype 2 Monte-Carlo\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.80, f\"1-10 particles\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.74, f\"{format_with_suffix(len(df_ma))} events\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "\n",
    "#ax.grid(True)\n",
    "fig.savefig(\"eval_eff_energy_many.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb01291-be64-40d8-9d25-0db2563f1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_single = df_ma[\"particles\"] == 1\n",
    "cnn_single = df_cnn[\"particles\"] == 1\n",
    "hdbscan_single = df_hdbscan[\"particles\"] == 1\n",
    "dbscan_single = df_dbscan[\"particles\"] == 1\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plot_performance_particle_num(df_ma[ma_single], \"avg_energy\", \"efficiency\", marker_shape=\"^\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_cnn[cnn_single], \"avg_energy\", \"efficiency\", marker_shape=\"s\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_hdbscan[hdbscan_single], \"avg_energy\", \"efficiency\", marker_shape=\"o\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_dbscan, \"avg_energy\", \"efficiency\", marker_shape=\"o\", linestyle=\"dashed\", ax=ax)\n",
    "#ax.set_xticks([1,2,3,4,5,6,7,8,9,10], [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\n",
    "ax.set_xticks([0,50,100,150,200,250,300,350], [\"0\",\"50\",\"100\",\"150\",\"200\",\"250\",\"300\",\"350\"])\n",
    "#ax.set_xticks([1,2], [\"1\",\"2\"])\n",
    "ax.set_ylim(0,4)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "#ax.yaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_ticks_position(\"both\")\n",
    "ax.xaxis.set_ticks_position(\"both\")\n",
    "ax.set_xlabel(\"Energy [GeV]\")\n",
    "ax.set_ylabel(\"Efficiency [$N_{clusters}/N_{particles}$]\")\n",
    "ax.axhline(1, color=\"grey\", linestyle=\"dotted\")\n",
    "ax.scatter([],[], label=\"Modified Aggregation\", marker=\"^\")\n",
    "ax.scatter([],[], label=\"CNN+Modified Aggregation\", marker=\"s\")\n",
    "ax.scatter([],[], label=\"HDBSCAN\", marker=\"o\")\n",
    "ax.scatter([],[], label=\"DBSCAN\", marker=\"o\")\n",
    "ax.legend(framealpha=0)\n",
    "\n",
    "ax.text(0.03, 0.93, \"FoCal-H\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\")\n",
    "ax.text(0.03, 0.86, \"Prototype 2 Monte-Carlo\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.80, f\"Single particles\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.74, f\"{format_with_suffix(len(df_ma[ma_single]))} events\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "fig.savefig(\"eval_eff_energy_single.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bae85-479d-43cf-974e-c3929929d9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "plot_performance_particle_num(df_ma, \"particles\", \"efficiency\", marker_shape=\"^\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_cnn, \"particles\", \"efficiency\", marker_shape=\"s\", linestyle=\"dashed\", ax=ax)\n",
    "plot_performance_particle_num(df_hdbscan, \"particles\", \"efficiency\", marker_shape=\"o\", linestyle=\"dashed\", ax=ax)\n",
    "ax.set_xticks([1,2,3,4,5,6,7,8,9,10], [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\n",
    "#ax.set_xticks([0,50,100,150,200,250,300,350], [\"0\",\"50\",\"100\",\"150\",\"200\",\"250\",\"300\",\"350\"])\n",
    "#ax.set_xticks([1,2], [\"1\",\"2\"])\n",
    "ax.set_ylim(0,3)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "#ax.yaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_ticks_position(\"both\")\n",
    "ax.xaxis.set_ticks_position(\"both\")\n",
    "ax.set_xlabel(\"Particles\")\n",
    "ax.set_ylabel(\"Efficiency [$N_{clusters}/N_{particles}$]\")\n",
    "ax.axhline(1, color=\"grey\", linestyle=\"dotted\")\n",
    "ax.scatter([],[], label=\"Modified Aggregation\", marker=\"^\")\n",
    "ax.scatter([],[], label=\"CNN+Modified Aggregation\", marker=\"s\")\n",
    "ax.scatter([],[], label=\"HDBSCAN\", marker=\"o\")\n",
    "ax.legend(framealpha=0)\n",
    "\n",
    "#ax.text(0.00, 1.05, \"FoCal-H Prototype 2\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\")\n",
    "#ax.text(1.00, 1.05, f\"${len(df_ma):.0E}$ MC Events\", transform=plt.gca().transAxes, ha=\"right\", va=\"center\")\n",
    "\n",
    "ax.text(0.03, 0.93, \"FoCal-H\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\")\n",
    "ax.text(0.03, 0.86, \"Prototype 2 Monte-Carlo\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.80, f\"Energy: 20-350 GeV\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.74, f\"{format_with_suffix(len(df_ma[ma_single]))} events\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "#ax.grid(True)\n",
    "fig.savefig(\"eval_eff_particles.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1f41b-cfa9-43f7-a1f7-386cf6ff87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_two = df_ma[\"particles\"] == 2\n",
    "cnn_two = df_cnn[\"particles\"] == 2\n",
    "hdbscan_two = df_hdbscan[\"particles\"] == 2\n",
    "bins=10\n",
    "fig,ax = plt.subplots()\n",
    "#plot_performance_particle_num(df_ma[ma_two], \"coms_dists\", \"vmeasure\", marker_shape=\"^\", linestyle=\"dashed\", ax=ax)\n",
    "plot_chunks(df_ma[ma_two][\"coms_dists\"], df_ma[ma_two][\"efficiency\"], bins, \"Modified Aggregation\", \"o\", \"\", ax)\n",
    "plot_chunks(df_cnn[cnn_two][\"coms_dists\"], df_cnn[cnn_two][\"efficiency\"], bins, \"CNN+Modified Aggregation\", \"o\", \"\", ax)\n",
    "plot_chunks(df_hdbscan[hdbscan_two][\"coms_dists\"], df_hdbscan[hdbscan_two][\"efficiency\"], bins, \"HDBSCAN\", \"o\", \"\", ax)\n",
    "\n",
    "ax.set_xticks(\n",
    "    [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20],\n",
    "    [\"0\", \"2\", \"4\", \"6\", \"8\", \"10\", \"12\", \"14\", \"16\", \"18\", \"20\"])\n",
    "ax.set_ylim(0,2)\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(1))\n",
    "#ax.yaxis.set_minor_locator(MultipleLocator(0.2))\n",
    "ax.yaxis.set_ticks_position(\"both\")\n",
    "ax.xaxis.set_ticks_position(\"both\")\n",
    "ax.set_xlabel(\"Center-of-mass distance [cm]\")\n",
    "ax.set_ylabel(\"Efficiency [$N_{clusters}/N_{particles}$]\")\n",
    "\n",
    "ax.scatter([],[], label=\"Modified Aggregation\", marker=\"^\")\n",
    "ax.scatter([],[], label=\"CNN+Modified Aggregation\", marker=\"s\")\n",
    "ax.scatter([],[], label=\"HDBSCAN\", marker=\"o\")\n",
    "ax.legend(framealpha=0)\n",
    "ax.axhline(1, color=\"grey\", linestyle=\"dotted\")\n",
    "\n",
    "ax.text(0.03, 0.93, \"FoCal-H\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\")\n",
    "ax.text(0.03, 0.86, \"Prototype 2 Monte-Carlo\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.80, f\"Energy: 20-350 GeV\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.74, f\"2-particle events\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "ax.text(0.03, 0.68, f\"{format_with_suffix(len(df_ma[ma_two]))} events\", transform=plt.gca().transAxes, ha=\"left\", va=\"center\", fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "fig.savefig(\"eval_eff_com.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16119b9d-da42-4d7c-8d83-313eea4fc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chunks(x, y, Nbins, label, marker_shape=\".\", color=\"\", ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    bins = np.linspace(min(x), max(x), Nbins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    mu_y = np.zeros(Nbins)\n",
    "    sigma_y = np.zeros(Nbins)\n",
    "    \n",
    "    for i in range(Nbins):\n",
    "        mask = (x >= bins[i]) & (x < bins[i+1])\n",
    "        if np.any(mask):\n",
    "            mu_y[i] = np.mean(y[mask])\n",
    "            sigma_y[i] = np.std(y[mask]) / np.sqrt(np.sum(mask))\n",
    "\n",
    "    if color == \"\":\n",
    "        ax.errorbar(bin_centers, mu_y, yerr=sigma_y, marker=marker_shape, linestyle='solid', capsize=5)\n",
    "    else:\n",
    "        ax.errorbar(bin_centers, mu_y, yerr=sigma_y, marker=marker_shape, linestyle='solid', color=color, capsize=5)\n",
    "\n",
    "\n",
    "def plot_separation(df,Nbins,ax):\n",
    "\n",
    "    x = df[\"coms_dists\"]\n",
    "    bins = np.linspace(min(x), max(x), Nbins + 1)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    mu_y = np.zeros(Nbins)\n",
    "    sigma_y = np.zeros(Nbins)\n",
    "\n",
    "\n",
    "    for i in range(Nbins):\n",
    "        mask = (x >= bins[i]) & (x < bins[i+1])\n",
    "        if np.any(mask):\n",
    "            mu_y[i] = df[mask][\"separation_resolved\"].sum() / df[mask][\"separation_total\"].sum()\n",
    "            print(df[mask][\"separation_resolved\"].sum())\n",
    "            #sigma_y[i] = #np.std(y[mask]) / np.sqrt(np.sum(mask))\n",
    "    \n",
    "    print(df[\"separation_resolved\"].mean())\n",
    "    ax.errorbar(bin_centers, mu_y, yerr=sigma_y, linestyle='solid', capsize=5)\n",
    "\n",
    "\n",
    "    pass\n",
    "\n",
    "two_mask = df_ma[\"particles\"] == 2\n",
    "en_mask = df_ma[\"avg_energy\"] == df_ma[\"avg_energy\"]\n",
    "eff_mask = df_ma[\"efficiency\"] == 1\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "#plot_chunks(df_ma[ma_two][\"coms_dists\"], df_ma[ma_two][\"efficiency\"], bins, \"Modified Aggregation\", \"o\", \"\", ax)\n",
    "\n",
    "plot_separation(df_ma[two_mask & eff_mask],10,ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54adc6-5e92-4dc5-99be-4931d89e37ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ma[two_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024814d-3061-40a3-8eb7-809c960a4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cnn[cnn_two]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26e64d-288a-4961-9491-33b6d7701178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.io import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b6f2f-f413-4685-8ee2-5eddbf0ab6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ma_eval[\"study\"].best_params)\n",
    "fig = optuna.visualization.plot_param_importances(ma_eval[\"study\"])\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e898577-8ecd-4032-87cb-fb22786411ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(cnn_eval[\"study\"][\"study\"])\n",
    "show(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
